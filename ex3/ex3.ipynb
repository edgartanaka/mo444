{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 3 (versao 1)\n",
    "- Leia o arquivo abalone do exercicio 2\n",
    "- faça o preprocessamento do atributo categorico e do atributo de saida como no exercicio 2\n",
    "- estandardize todos os atributos numéricos. Voce pode estardartizar todo o arquivo de uma vez. Como discutimos em aula esse não é a coisa 100% certa, mas é um erro menor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model, preprocessing, svm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# Constants\n",
    "costs = 10 ** np.arange(-1., 4.)\n",
    "\n",
    "# Pre-processing\n",
    "def get_data():\n",
    "    df = pd.read_csv('abalone.csv', header=None)\n",
    "    \n",
    "    # convert feature from numerical to categorical\n",
    "    y = np.where(df.ix[:,8] > 13, 1,0)\n",
    "\n",
    "    # drop target column\n",
    "    x = df.drop(df.columns[[8]], axis=1)\n",
    "\n",
    "    # one hot encode gender feature\n",
    "    x = pd.get_dummies(x)    \n",
    "    \n",
    "    # standardization \n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    x = scaler.fit_transform(x)\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "# Total correct predictions divided by total number of samples\n",
    "def accuracy(predicted, y):    \n",
    "    return np.sum(predicted == y) / y.shape[0]\n",
    "\n",
    "X, y = get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "- faça o logistic regression com $C=10^{-1,0,1,2,3}$. O loop externo deve ser um 5-fold CV estratificado. \n",
    "- O loop interno para a escolha do hiperparametro deve ser um 3-fold estratificado.\n",
    "- voce tem que fazer o loop interno explicitamente, usando StratifiedKFold e não funções como GridSearchCV\n",
    "- qual a acurácia do LR com a melhore escolha de parametros (para cada fold)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best C is 1.0 with accuracy 0.895933014354\n",
      "best C is 1000.0 with accuracy 0.894736842105\n",
      "best C is 100.0 with accuracy 0.899401197605\n",
      "best C is 1000.0 with accuracy 0.900598802395\n",
      "best C is 100.0 with accuracy 0.895808383234\n",
      "mean accuracy is 0.897295647939\n"
     ]
    }
   ],
   "source": [
    "def grid_search(X, y):\n",
    "    kfold = StratifiedKFold(n_splits=3, random_state=1)    \n",
    "    accuracies = [0, 0, 0, 0, 0]\n",
    "    \n",
    "    # 3 fold to find the best C\n",
    "    for train_index, test_index in kfold.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "        for i in range(len(costs)):\n",
    "            logistic = linear_model.LogisticRegression(random_state=1, C=costs[i])\n",
    "            logistic.fit(X_train, y_train)\n",
    "            acc = logistic.score(X_test, y_test)\n",
    "            accuracies[i] += acc\n",
    "    \n",
    "    # find highest accuracy and return the C for that accuracy\n",
    "    highest_acc = max(accuracies)\n",
    "    highest_acc_index = accuracies.index(highest_acc)\n",
    "    return costs[highest_acc_index]\n",
    "    \n",
    "X, y = get_data()\n",
    "outter = StratifiedKFold(n_splits=5, random_state=1)\n",
    "accs = []\n",
    "\n",
    "# outer loop\n",
    "# 5 fold to calculate accuracy for this model\n",
    "for train_index, test_index in outter.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # inner loop\n",
    "    # pick the best cost hyper parameter\n",
    "    c = grid_search(X_train, y_train)\n",
    "    \n",
    "    logistic = linear_model.LogisticRegression(random_state=1, C=c)\n",
    "    logistic.fit(X_train, y_train)    \n",
    "    acc = logistic.score(X_test, y_test)\n",
    "    accs.append(acc)\n",
    "    print('best C is', c, 'with accuracy', acc)\n",
    "\n",
    "lr_acc = np.mean(accs)\n",
    "print('mean accuracy is', lr_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVM\n",
    "- faça o LinearSVM com $C=10^{-1,0,1,2,3}$. O loop externo deve ser um 5-fold estratificado. \n",
    "- O loop interno um 3-fold estratificado. Neste caso voce nao precisa fazer o 3 fold explicitamente, \n",
    "- voce pode usar o GridSearchCV.\n",
    "- qual a acurácia do LinearSVM com a melhor escolha de C?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best C is 10.0 with accuracy 0.8995215311\n",
      "best C is 10.0 with accuracy 0.897129186603\n",
      "best C is 0.1 with accuracy 0.901796407186\n",
      "best C is 1.0 with accuracy 0.899401197605\n",
      "best C is 0.1 with accuracy 0.893413173653\n",
      "mean accuracy of svm is 0.898252299229\n"
     ]
    }
   ],
   "source": [
    "X, y = get_data()\n",
    "outter = StratifiedKFold(n_splits=5, random_state=1)\n",
    "accs = []\n",
    "\n",
    "# 5 fold to calculate accuracy for this model\n",
    "for train_index, test_index in outter.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    parameters = {'C':costs}\n",
    "    svr = svm.LinearSVC()\n",
    "    clf = GridSearchCV(svr, parameters, cv=3)\n",
    "    model = clf.fit(X_train, y_train)\n",
    "    \n",
    "    svr = svm.LinearSVC(C=clf.best_params_['C'])\n",
    "    svr.fit(X_train, y_train)\n",
    "    acc = svr.score(X_test, y_test)\n",
    "    accs.append(acc)\n",
    "    print('best C is', clf.best_params_['C'], 'with accuracy', acc)\n",
    "\n",
    "svm_acc = np.mean(accs)\n",
    "print('mean accuracy of svm is', svm_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA\n",
    "Faça o LDA. Reporte a acuracia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.893540669856\n",
      "accuracy 0.898325358852\n",
      "accuracy 0.898203592814\n",
      "accuracy 0.897005988024\n",
      "accuracy 0.891017964072\n",
      "mean accuracy = 0.891017964072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tanakaed/anaconda3/envs/condaenv/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:387: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    }
   ],
   "source": [
    "X, y = get_data()\n",
    "outter = StratifiedKFold(n_splits=5, random_state=1)\n",
    "accs = []\n",
    "for train_index, test_index in outter.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    clf = LinearDiscriminantAnalysis()\n",
    "    clf.fit(X_train, y_train)    \n",
    "    acc = clf.score(X_test, y_test)\n",
    "    accs.append(acc)\n",
    "    print('accuracy', acc)\n",
    "\n",
    "lda_acc = np.mean(acc)\n",
    "print('mean accuracy =', lda_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificador Final\n",
    "- qual o melhor classificador para esse problema?\n",
    "- se não o LDA, calcule o hiperparametro C a ser usado\n",
    "- gere o classificador final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAc8AAAEWCAYAAAAASRzMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzF9iFTzRVeoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114a8f0b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Example data\n",
    "algos = ('LogReg', 'SVM','LDA')\n",
    "y_pos = np.arange(len(algos))\n",
    "accs = [lr_acc, svm_acc, lda_acc]\n",
    "\n",
    "ax.barh(y_pos, accs, align='center')\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(algos)\n",
    "ax.invert_yaxis()  # labels read top-to-bottom\n",
    "ax.set_xlabel('Accuracy')\n",
    "ax.set_title('Model Comparison')\n",
    "for i, v in enumerate(accs):\n",
    "    ax.text(v, i, str(v))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model is SVM with C is 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = get_data()\n",
    "\n",
    "# pick the best cost hyper parameter\n",
    "parameters = {'C':costs}\n",
    "\n",
    "model_accs = [lr_acc, svm_acc, lda_acc]\n",
    "highest_acc = max(model_accs)\n",
    "highest_acc_index = model_accs.index(highest_acc)\n",
    "final_model = None\n",
    "\n",
    "if highest_acc_index == 2:\n",
    "    print('best model is LDA')\n",
    "    clf = LinearDiscriminantAnalysis()\n",
    "    final_model = clf.fit(X, y)\n",
    "elif highest_acc_index == 1:\n",
    "    clf = GridSearchCV(svm.LinearSVC(), parameters, cv=3)\n",
    "    model = clf.fit(X, y)\n",
    "    c = clf.best_params_['C']\n",
    "    print('best model is SVM with C is', c)\n",
    "    final_model = svm.LinearSVC(C=c)   \n",
    "else:\n",
    "    clf = GridSearchCV(linear_model.LogisticRegression(random_state=1), parameters, cv=3)\n",
    "    model = clf.fit(X, y)\n",
    "    c = clf.best_params_['C']\n",
    "    print('best model is Logistic Regression with C is', c)\n",
    "    final_model = linear_model.LogisticRegression(random_state=1, C=c)\n",
    "    \n",
    "# this is the final model to be delivered and go to production\n",
    "final_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
